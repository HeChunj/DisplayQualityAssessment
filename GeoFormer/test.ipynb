{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "\n",
    "def crop_image(image_path, centers, window_size, model_name=\"\"):\n",
    "    \"\"\"\n",
    "    image_path: 图片路径\n",
    "    centers: 一系列中心坐标 (x, y) 的列表\n",
    "    window_size: 切割图块的大小（正方形的边长）\n",
    "    \"\"\"\n",
    "    # 检查图像路径是否存在\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    half_window = window_size // 2  # 窗口的一半\n",
    "\n",
    "    # 记录开始的时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 记录裁剪的图块的地址\n",
    "    cropped_image_paths = []\n",
    "\n",
    "    # 获取当前工作目录\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # 提取文件名（不带扩展名）\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    result_dir = f'{current_directory}/croped_result_SONY/{model_name}/{filename_without_ext}'\n",
    "\n",
    "    # 检查目录是否存在，如果不存在则创建\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    for i, (x, y) in enumerate(centers):\n",
    "        # 裁剪图块\n",
    "        cropped_image = crop_and_save(\n",
    "            image_path, image, x, y, half_window, width, height, i, result_dir)\n",
    "        cropped_image_paths.append(cropped_image)\n",
    "    # # 使用多线程处理裁剪和保存\n",
    "    # with ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    #     futures = []\n",
    "    #     :\n",
    "    #         futures.append(\n",
    "    #             executor.submit(crop_and_save, image_path, image, x, y,\n",
    "    #                             half_window, width, height, i, result_dir)\n",
    "    #         )\n",
    "\n",
    "    #     # 获取结果\n",
    "    #     for future in futures:\n",
    "    #         cropped_image_paths.append(future.result())\n",
    "\n",
    "    # 记录结束的时间\n",
    "    end_time = time.time()\n",
    "    print(f\"Time elapsed, crop img: {end_time - start_time} seconds\")\n",
    "\n",
    "    return cropped_image_paths\n",
    "\n",
    "\n",
    "def crop_and_save(image_path, image, x, y, half_window, width, height, i, result_dir):\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # 确定裁剪区域的左上角和右下角\n",
    "    left = max(0, x - half_window)\n",
    "    top = max(0, y - half_window)\n",
    "    right = min(width, x + half_window)\n",
    "    bottom = min(height, y + half_window)\n",
    "\n",
    "    left = round(left)\n",
    "    top = round(top)\n",
    "    right = round(right)\n",
    "    bottom = round(bottom)\n",
    "\n",
    "    # 裁剪图块\n",
    "    cropped_image = image[top:bottom, left:right]\n",
    "\n",
    "    # 保存裁剪的图块的坐标信息到json文件\n",
    "    json_path = os.path.join(result_dir, f\"cropped_image_coordinate.json\")\n",
    "    res = {}\n",
    "    res[\"coordinates\"] = []\n",
    "    res[\"coordinates\"].append({\n",
    "        \"cx\": x,\n",
    "        \"cy\": y,\n",
    "        \"left\": left,\n",
    "        \"top\": top,\n",
    "        \"right\": right,\n",
    "        \"bottom\": bottom\n",
    "    })\n",
    "    if os.path.exists(json_path):\n",
    "        # 删除已经存在的json文件\n",
    "        # os.remove(json_path)\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        res['coordinates'] = res['coordinates'] + data['coordinates']\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(res, json_file, indent=4)\n",
    "\n",
    "    # 保存裁剪的图块到指定目录\n",
    "    save_path = os.path.join(result_dir, f\"cropped_image_{i}.png\")\n",
    "    # cv2.imwrite(save_path, cropped_image)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize GeoFormer_geoformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45690/1833660384.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt_dict = torch.load(ckpt, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import rawpy\n",
    "import os\n",
    "\n",
    "from model.loftr_src.loftr.utils.cvpr_ds_config import default_cfg\n",
    "from model.full_model import GeoFormer as GeoFormer_\n",
    "\n",
    "from eval_tool.immatch.utils.data_io import load_gray_scale_tensor_cv\n",
    "from model.geo_config import default_cfg as geoformer_cfg\n",
    "\n",
    "\n",
    "class GeoFormer():\n",
    "    def __init__(self, imsize, match_threshold, no_match_upscale=False, ckpt=None, device='cuda'):\n",
    "\n",
    "        self.device = device\n",
    "        self.imsize = imsize\n",
    "        self.match_threshold = match_threshold\n",
    "        self.no_match_upscale = no_match_upscale\n",
    "\n",
    "        # Load model\n",
    "        conf = dict(default_cfg)\n",
    "        conf['match_coarse']['thr'] = self.match_threshold\n",
    "        geoformer_cfg['coarse_thr'] = self.match_threshold\n",
    "        self.model = GeoFormer_(conf)\n",
    "        ckpt_dict = torch.load(ckpt, map_location=torch.device('cpu'))\n",
    "        if 'state_dict' in ckpt_dict:\n",
    "            ckpt_dict = ckpt_dict['state_dict']\n",
    "        self.model.load_state_dict(ckpt_dict, strict=False)\n",
    "        self.model = self.model.eval().to(self.device)\n",
    "\n",
    "        # Name the method\n",
    "        self.ckpt_name = ckpt.split('/')[-1].split('.')[0]\n",
    "        self.name = f'GeoFormer_{self.ckpt_name}'\n",
    "        if self.no_match_upscale:\n",
    "            self.name += '_noms'\n",
    "        print(f'Initialize {self.name}')\n",
    "\n",
    "    def change_deivce(self, device):\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_im(self, im_path, enhanced=False):\n",
    "        return load_gray_scale_tensor_cv(\n",
    "            im_path, self.device, imsize=self.imsize, dfactor=8, enhanced=enhanced, value_to_scale=min\n",
    "        )\n",
    "\n",
    "    def match_inputs_(self, gray1, gray2, is_draw=False):\n",
    "\n",
    "        batch = {'image0': gray1, 'image1': gray2}\n",
    "        with torch.no_grad():\n",
    "            batch = self.model(batch)\n",
    "        kpts1 = batch['mkpts0_f'].cpu().numpy()\n",
    "        kpts2 = batch['mkpts1_f'].cpu().numpy()\n",
    "\n",
    "        def draw():\n",
    "            import matplotlib.pyplot as plt\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "            plt.figure(dpi=500)\n",
    "            kp0 = kpts1\n",
    "            kp1 = kpts2\n",
    "            # if len(kp0) > 0:\n",
    "            kp0 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kp0]\n",
    "            kp1 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kp1]\n",
    "            matches = [cv2.DMatch(_trainIdx=i, _queryIdx=i, _distance=1, _imgIdx=-1) for i in\n",
    "                       range(len(kp0))]\n",
    "\n",
    "            show = cv2.drawMatches((gray1.cpu()[0][0].numpy() * 255).astype(np.uint8), kp0,\n",
    "                                   (gray2.cpu()[0][0].numpy() *\n",
    "                                    255).astype(np.uint8), kp1, matches,\n",
    "                                   None)\n",
    "            plt.imshow(show)\n",
    "            plt.show()\n",
    "        if is_draw:\n",
    "            draw()\n",
    "        scores = batch['mconf'].cpu().numpy()\n",
    "        matches = np.concatenate([kpts1, kpts2], axis=1)\n",
    "        return matches, kpts1, kpts2, scores\n",
    "\n",
    "    def match_pairs(self, im1_path, im2_path, cpu=False, is_draw=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        tmp_device = self.device\n",
    "        if cpu:\n",
    "            self.change_deivce('cpu')\n",
    "\n",
    "        gray1, sc1 = self.load_im(im1_path)\n",
    "        gray2, sc2 = self.load_im(im2_path)\n",
    "\n",
    "        upscale = np.array([sc1 + sc2])\n",
    "        matches, kpts1, kpts2, scores = self.match_inputs_(\n",
    "            gray1, gray2, is_draw)\n",
    "\n",
    "        if self.no_match_upscale:\n",
    "            return matches, kpts1, kpts2, scores, upscale.squeeze(0)\n",
    "\n",
    "        # Upscale matches &  kpts\n",
    "        matches = upscale * matches\n",
    "        kpts1 = sc1 * kpts1\n",
    "        kpts2 = sc2 * kpts2\n",
    "\n",
    "        if cpu:\n",
    "            self.change_deivce(tmp_device)\n",
    "\n",
    "        return matches, kpts1, kpts2, scores\n",
    "\n",
    "\n",
    "g = GeoFormer(640, 0.9, no_match_upscale=False,\n",
    "              ckpt='saved_ckpt/geoformer.ckpt', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 36.98241639137268 seconds\n",
      "Time elapsed, crop img: 57.119438886642456 seconds\n",
      "455 455\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 25.975215911865234 seconds\n",
      "Time elapsed, crop img: 40.53769040107727 seconds\n",
      "336 336\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 78.51464176177979 seconds\n",
      "Time elapsed, crop img: 121.85646510124207 seconds\n",
      "965 965\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 8.931729078292847 seconds\n",
      "Time elapsed, crop img: 14.349292278289795 seconds\n",
      "125 125\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 33.812220335006714 seconds\n",
      "Time elapsed, crop img: 54.722655057907104 seconds\n",
      "439 439\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 84.1193540096283 seconds\n",
      "Time elapsed, crop img: 131.4725844860077 seconds\n",
      "1032 1032\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 17.253784894943237 seconds\n",
      "Time elapsed, crop img: 27.546655893325806 seconds\n",
      "229 229\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 30.65032958984375 seconds\n",
      "Time elapsed, crop img: 50.03079676628113 seconds\n",
      "431 431\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 2.8656630516052246 seconds\n",
      "Time elapsed, crop img: 4.617250204086304 seconds\n",
      "41 41\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 3.015216827392578 seconds\n",
      "Time elapsed, crop img: 4.888324975967407 seconds\n",
      "42 42\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 12.69236135482788 seconds\n",
      "Time elapsed, crop img: 20.420934438705444 seconds\n",
      "171 171\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 41.08662176132202 seconds\n",
      "Time elapsed, crop img: 64.14609670639038 seconds\n",
      "538 538\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 45.09998178482056 seconds\n",
      "Time elapsed, crop img: 70.5159547328949 seconds\n",
      "601 601\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 48.5897479057312 seconds\n",
      "Time elapsed, crop img: 77.29053258895874 seconds\n",
      "613 613\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 15.531015634536743 seconds\n",
      "Time elapsed, crop img: 25.514196395874023 seconds\n",
      "222 222\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 32.50721764564514 seconds\n",
      "Time elapsed, crop img: 50.55620312690735 seconds\n",
      "424 424\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 14.826598167419434 seconds\n",
      "Time elapsed, crop img: 23.618303775787354 seconds\n",
      "194 194\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 3.422771453857422 seconds\n",
      "Time elapsed, crop img: 5.588791608810425 seconds\n",
      "49 49\n",
      "Loading normal image...\n",
      "Loading normal image...\n",
      "Time elapsed, crop img: 3.3855438232421875e-05 seconds\n",
      "Time elapsed, crop img: 4.0531158447265625e-05 seconds\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    img1_path = f\"/home/hechunjiang/gradio/样品1 LG 65UF8580/华为P50手机采集图像/监视器采集图像/{i}.jpg\"\n",
    "    img2_path = f\"/home/hechunjiang/gradio/样品2 SONY 43吋/华为P50手机采集图像/{i}.jpg\"\n",
    "    # img2_path = f\"/home/hechunjiang/gradio/样品3 亚马逊 43吋/华为P50手机采集图像/{i}.jpg\"\n",
    "    # img2_path = f\"/home/hechunjiang/gradio/样品1 LG 65UF8580/华为P50手机采集图像/样品1采集图像/{i}.jpg\"\n",
    "    \n",
    "    # 华为p50\n",
    "    matches, kpts1, kpts2, scores = g.match_pairs(\n",
    "        img1_path, img2_path, is_draw=False)\n",
    "    res = {\"matches\": matches.tolist(), \"kpts1\": kpts1.tolist(),\n",
    "           \"kpts2\": kpts2.tolist(), \"scores\": scores.tolist()}\n",
    "    croped_image_path1 = crop_image(\n",
    "        img1_path, res['kpts1'], 300, \"finetune_ref\")\n",
    "    croped_image_path2 = crop_image(\n",
    "        img2_path, res['kpts2'], 300, \"finetune_dst\")\n",
    "    print(len(res['kpts1']), len(res['kpts2']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
