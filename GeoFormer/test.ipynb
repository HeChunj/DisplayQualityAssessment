{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import Image\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def crop_image(image_path, centers, window_size, demo=\"\", data_type=\"\"):\n",
    "    \"\"\"\n",
    "    image_path: 图片路径\n",
    "    centers: 一系列中心坐标 (x, y) 的列表\n",
    "    window_size: 切割图块的大小（正方形的边长）\n",
    "    \"\"\"\n",
    "\n",
    "    # 检查图像路径是否存在\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    half_window = window_size // 2  # 窗口的一半\n",
    "\n",
    "    # 记录开始的时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 记录裁剪的图块的地址\n",
    "    cropped_image_paths = []\n",
    "\n",
    "    # 获取当前工作目录\n",
    "    current_directory = os.getcwd()\n",
    "\n",
    "    # 提取文件名（不带扩展名）\n",
    "    filename_without_ext = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "    result_dir = f'{current_directory}/croped_result_{demo}/{data_type}/{filename_without_ext}'\n",
    "\n",
    "    # 检查目录是否存在，如果不存在则创建\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    for i, (x, y) in enumerate(centers):\n",
    "        # 裁剪图块\n",
    "        cropped_image = crop_and_save(\n",
    "            image, x, y, half_window, width, height, i, result_dir)\n",
    "        cropped_image_paths.append(cropped_image)\n",
    "\n",
    "    # 记录结束的时间\n",
    "    end_time = time.time()\n",
    "    # print(f\"Time elapsed, crop img: {end_time - start_time} seconds\")\n",
    "\n",
    "    return cropped_image_paths\n",
    "\n",
    "\n",
    "def crop_and_save(image, x, y, half_window, width, height, i, result_dir):\n",
    "\n",
    "    # 确定裁剪区域的左上角和右下角\n",
    "    left = max(0, x - half_window)\n",
    "    top = max(0, y - half_window)\n",
    "    right = min(width, x + half_window)\n",
    "    bottom = min(height, y + half_window)\n",
    "\n",
    "    left = round(left)\n",
    "    top = round(top)\n",
    "    right = round(right)\n",
    "    bottom = round(bottom)\n",
    "\n",
    "    # 确保裁剪区域在图像边界内\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    if top < 0:\n",
    "        top = 0\n",
    "    if right > width:\n",
    "        right = width\n",
    "    if bottom > height:\n",
    "        bottom = height\n",
    "\n",
    "    # 裁剪图块\n",
    "    cropped_image = image[top:bottom, left:right]\n",
    "\n",
    "    # 保存裁剪的图块的坐标信息到json文件\n",
    "    json_path = os.path.join(result_dir, f\"cropped_image_coordinate.json\")\n",
    "    res = {}\n",
    "    res[\"coordinates\"] = []\n",
    "    res[\"coordinates\"].append({\n",
    "        \"cx\": x,\n",
    "        \"cy\": y,\n",
    "        \"left\": left,\n",
    "        \"top\": top,\n",
    "        \"right\": right,\n",
    "        \"bottom\": bottom\n",
    "    })\n",
    "    save = {}\n",
    "    save[\"coordinates\"] = []\n",
    "\n",
    "    if os.path.exists(json_path):\n",
    "        # 删除已经存在的json文件\n",
    "        # os.remove(json_path)\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "    else:\n",
    "        data = {}\n",
    "        data['coordinates'] = []\n",
    "    save['coordinates'] = data['coordinates'] + res['coordinates']\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(save, json_file, indent=4)\n",
    "\n",
    "    # 保存裁剪的图块到指定目录\n",
    "    save_path = os.path.join(result_dir, f\"cropped_image_{i}.png\")\n",
    "    cv2.imwrite(save_path, cropped_image)\n",
    "\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize GeoFormer_geoformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200233/1758442171.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt_dict = torch.load(ckpt, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import rawpy\n",
    "import os\n",
    "\n",
    "from model.loftr_src.loftr.utils.cvpr_ds_config import default_cfg\n",
    "from model.full_model import GeoFormer as GeoFormer_\n",
    "\n",
    "from eval_tool.immatch.utils.data_io import load_gray_scale_tensor_cv\n",
    "from model.geo_config import default_cfg as geoformer_cfg\n",
    "\n",
    "\n",
    "class GeoFormer():\n",
    "    def __init__(self, imsize, match_threshold, no_match_upscale=False, ckpt=None, device='cuda'):\n",
    "\n",
    "        self.device = device\n",
    "        self.imsize = imsize\n",
    "        self.match_threshold = match_threshold\n",
    "        self.no_match_upscale = no_match_upscale\n",
    "\n",
    "        # Load model\n",
    "        conf = dict(default_cfg)\n",
    "        conf['match_coarse']['thr'] = self.match_threshold\n",
    "        geoformer_cfg['coarse_thr'] = self.match_threshold\n",
    "        self.model = GeoFormer_(conf)\n",
    "        ckpt_dict = torch.load(ckpt, map_location=torch.device('cpu'))\n",
    "        if 'state_dict' in ckpt_dict:\n",
    "            ckpt_dict = ckpt_dict['state_dict']\n",
    "        self.model.load_state_dict(ckpt_dict, strict=False)\n",
    "        self.model = self.model.eval().to(self.device)\n",
    "\n",
    "        # Name the method\n",
    "        self.ckpt_name = ckpt.split('/')[-1].split('.')[0]\n",
    "        self.name = f'GeoFormer_{self.ckpt_name}'\n",
    "        if self.no_match_upscale:\n",
    "            self.name += '_noms'\n",
    "        print(f'Initialize {self.name}')\n",
    "\n",
    "    def change_deivce(self, device):\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_im(self, im_path, enhanced=False):\n",
    "        return load_gray_scale_tensor_cv(\n",
    "            im_path, self.device, imsize=self.imsize, dfactor=8, enhanced=enhanced, value_to_scale=min\n",
    "        )\n",
    "\n",
    "    def match_inputs_(self, gray1, gray2, is_draw=False):\n",
    "\n",
    "        batch = {'image0': gray1, 'image1': gray2}\n",
    "        with torch.no_grad():\n",
    "            batch = self.model(batch)\n",
    "        kpts1 = batch['mkpts0_f'].cpu().numpy()\n",
    "        kpts2 = batch['mkpts1_f'].cpu().numpy()\n",
    "\n",
    "        def draw():\n",
    "            import matplotlib.pyplot as plt\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "            plt.figure(dpi=500)\n",
    "            kp0 = kpts1\n",
    "            kp1 = kpts2\n",
    "            # if len(kp0) > 0:\n",
    "            kp0 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kp0]\n",
    "            kp1 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kp1]\n",
    "            matches = [cv2.DMatch(_trainIdx=i, _queryIdx=i, _distance=1, _imgIdx=-1) for i in\n",
    "                       range(len(kp0))]\n",
    "\n",
    "            show = cv2.drawMatches((gray1.cpu()[0][0].numpy() * 255).astype(np.uint8), kp0,\n",
    "                                   (gray2.cpu()[0][0].numpy() *\n",
    "                                    255).astype(np.uint8), kp1, matches,\n",
    "                                   None)\n",
    "            plt.imshow(show)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        if is_draw:\n",
    "            draw()\n",
    "        scores = batch['mconf'].cpu().numpy()\n",
    "        matches = np.concatenate([kpts1, kpts2], axis=1)\n",
    "        return matches, kpts1, kpts2, scores\n",
    "\n",
    "    def match_pairs(self, im1_path, im2_path, cpu=False, is_draw=False):\n",
    "        torch.cuda.empty_cache()\n",
    "        tmp_device = self.device\n",
    "        if cpu:\n",
    "            self.change_deivce('cpu')\n",
    "\n",
    "        gray1, sc1 = self.load_im(im1_path)\n",
    "        gray2, sc2 = self.load_im(im2_path)\n",
    "\n",
    "        upscale = np.array([sc1 + sc2])\n",
    "        matches, kpts1, kpts2, scores = self.match_inputs_(\n",
    "            gray1, gray2, is_draw)\n",
    "\n",
    "        if self.no_match_upscale:\n",
    "            return matches, kpts1, kpts2, scores, upscale.squeeze(0)\n",
    "\n",
    "        # Upscale matches &  kpts\n",
    "        matches = upscale * matches\n",
    "        kpts1 = sc1 * kpts1\n",
    "        kpts2 = sc2 * kpts2\n",
    "\n",
    "        if cpu:\n",
    "            self.change_deivce(tmp_device)\n",
    "\n",
    "        return matches, kpts1, kpts2, scores\n",
    "\n",
    "\n",
    "g = GeoFormer(640, 0.5, no_match_upscale=False,\n",
    "              ckpt='saved_ckpt/geoformer.ckpt', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_homography_res(img1_path, img2_path, kpts1, kpts2, matches, is_draw=False):\n",
    "    if len(matches) < 4:\n",
    "        return None, None\n",
    "\n",
    "    img1_color = cv2.imread(img1_path)\n",
    "    img2_color = cv2.imread(img2_path)\n",
    "\n",
    "    src_pts = np.float32(matches[:, :2]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32(matches[:, 2:]).reshape(-1, 1, 2)\n",
    "\n",
    "    matrix, inliers = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    height, width = img2_color.shape[:2]\n",
    "\n",
    "    aligned_img1_color = cv2.warpPerspective(\n",
    "        img1_color, matrix, (width, height))\n",
    "\n",
    "    if is_draw:\n",
    "        plt.figure(dpi=500)\n",
    "\n",
    "        # 示例关键点和图像\n",
    "        kp0 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kpts2]\n",
    "        kp1 = [cv2.KeyPoint(int(k[0]), int(k[1]), 30) for k in kpts2]\n",
    "        matches = [cv2.DMatch(_trainIdx=i, _queryIdx=i,\n",
    "                              _distance=1, _imgIdx=-1) for i in range(len(kp0))]\n",
    "\n",
    "        # 转换图像为 RGB 格式\n",
    "        aligned_img1_rgb = cv2.cvtColor(aligned_img1_color, cv2.COLOR_BGR2RGB)\n",
    "        img2_rgb = cv2.cvtColor(img2_color, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 创建拼接图像\n",
    "        h1, w1 = aligned_img1_rgb.shape[:2]\n",
    "        h2, w2 = img2_rgb.shape[:2]\n",
    "        canvas = np.zeros((max(h1, h2), w1 + w2, 3), dtype=np.uint8)\n",
    "        canvas[:h1, :w1] = aligned_img1_rgb\n",
    "        canvas[:h2, w1:w1 + w2] = img2_rgb\n",
    "\n",
    "        # 绘制匹配点和随机颜色连线\n",
    "        for match in matches:\n",
    "            pt1 = (int(kp0[match.queryIdx].pt[0]),\n",
    "                   int(kp0[match.queryIdx].pt[1]))\n",
    "            pt2 = (int(kp1[match.trainIdx].pt[0]) +\n",
    "                   w1, int(kp1[match.trainIdx].pt[1]))\n",
    "            random_color = tuple(np.random.randint(0, 256, 3).tolist())\n",
    "            cv2.line(canvas, pt1, pt2, color=random_color, thickness=2)\n",
    "            cv2.circle(canvas, pt1, radius=10,\n",
    "                       color=random_color, thickness=-1)\n",
    "            cv2.circle(canvas, pt2, radius=10,\n",
    "                       color=random_color, thickness=-1)\n",
    "\n",
    "        # 显示结果\n",
    "        print(\"仿射变换后的匹配结果：\")\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return cv2.cvtColor(aligned_img1_color, cv2.COLOR_BGR2RGB), matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "def transform_coord(demo, i, matrix):\n",
    "\n",
    "    # 读取 attention.json 文件\n",
    "    with open(f'/home/hechunjiang/gradio/src/result/attention_area/{i}.json', 'r') as f:\n",
    "        attention_data = json.load(f)\n",
    "\n",
    "    # 提取关注区域坐标\n",
    "    points = attention_data['points']\n",
    "\n",
    "    # 变换区域坐标\n",
    "    transformed_points = []\n",
    "    for point in points:\n",
    "        # 获取原始区域的四个角坐标\n",
    "        x1, y1, x2, y2 = point['x1'], point['y1'], point['x2'], point['y2']\n",
    "\n",
    "        # 使用变换矩阵将区域的四个角坐标变换到新图像坐标系\n",
    "        corners = np.array([[x1, y1], [x2, y1], [x2, y2], [x1, y2]], dtype='float32').reshape(-1, 1, 2)\n",
    "        transformed_corners = cv2.perspectiveTransform(corners, matrix)\n",
    "\n",
    "        # 获取变换后的矩形区域的坐标\n",
    "        # transformed_corners 是一个 4x1x2 的数组，需要提取并计算新的 x1, y1, x2, y2\n",
    "        x_min = float(min(transformed_corners[0][0][0], transformed_corners[1][0][0], transformed_corners[2][0][0], transformed_corners[3][0][0]))\n",
    "        y_min = float(min(transformed_corners[0][0][1], transformed_corners[1][0][1], transformed_corners[2][0][1], transformed_corners[3][0][1]))\n",
    "        x_max = float(max(transformed_corners[0][0][0], transformed_corners[1][0][0], transformed_corners[2][0][0], transformed_corners[3][0][0]))\n",
    "        y_max = float(max(transformed_corners[0][0][1], transformed_corners[1][0][1], transformed_corners[2][0][1], transformed_corners[3][0][1]))\n",
    "\n",
    "        # 更新为变换后的矩形区域\n",
    "        transformed_points.append({\n",
    "            'x1': x_min,\n",
    "            'y1': y_min,\n",
    "            'x2': x_max,\n",
    "            'y2': y_max\n",
    "        })\n",
    "\n",
    "    # 将变换后的结果保存回 JSON 文件\n",
    "    attention_data['points'] = transformed_points\n",
    "\n",
    "    if not os.path.exists(f'/home/hechunjiang/gradio/src/result/attention_area/{demo}'):\n",
    "        os.makedirs(f'/home/hechunjiang/gradio/src/result/attention_area/{demo}')\n",
    "\n",
    "    with open(f'/home/hechunjiang/gradio/src/result/attention_area/{demo}/transformed_attention_{i}.json', 'w') as f:\n",
    "        json.dump(attention_data, f, indent=4)\n",
    "\n",
    "    print(f\"Transformed attention area {demo} {i} and saved to {demo}/transformed_attention_{i}.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# 删除/home/hechunjiang/gradio/GeoFormer/croped_result_KTC/finetune_dst/19\n",
    "demo_list = [\"LG\", \"SONY\", \"AMAZON\", \"SKYWORTH\", \"KTC\", \"REDMAGIC\", \"HISENSE\"]\n",
    "\n",
    "for demo in demo_list:\n",
    "    for i in [19, 20, 21]:\n",
    "        if os.path.exists(f'/home/hechunjiang/gradio/GeoFormer/croped_result_{demo}/finetune_dst/{i}'):\n",
    "            shutil.rmtree(f'/home/hechunjiang/gradio/GeoFormer/croped_result_{demo}/finetune_dst/{i}')\n",
    "        if os.path.exists(f'/home/hechunjiang/gradio/GeoFormer/croped_result_{demo}/finetune_ref/{i}'):\n",
    "            shutil.rmtree(f'/home/hechunjiang/gradio/GeoFormer/croped_result_{demo}/finetune_ref/{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "demo_list = [\"LG\", \"SONY\", \"AMAZON\", \"SKYWORTH\", \"KTC\", \"REDMAGIC\", \"HISENSE\"]\n",
    "\n",
    "img2_path_list = [\n",
    "    \"/home/hechunjiang/gradio/样品1 LG 65UF8580/华为P50手机采集图像/样品1采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品2 SONY 43吋/华为P50手机采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品3 亚马逊 43吋/华为P50手机采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品4 创维 F32D80U/华为P50采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品5 KTC M27P20P/华为P50采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品6 红魔 GM001S/华为P50采集图像/\",\n",
    "    \"/home/hechunjiang/gradio/样品7 海信 27G7K-PRO/华为P50采集图像/\"\n",
    "]\n",
    "\n",
    "all_croped_image_counts = [[] for _ in range(len(demo_list))]\n",
    "\n",
    "for idx, (demo, image2_path) in enumerate(zip(demo_list, img2_path_list)):\n",
    "    for i in range(19, 22):\n",
    "    # for i in [9, 10, 18]:\n",
    "        img1_path = f\"/home/hechunjiang/gradio/监视器采集图像/{i}.jpg\"\n",
    "        img2_path = f\"{image2_path}{i}.jpg\"\n",
    "\n",
    "        matches, kpts1, kpts2, scores = g.match_pairs(\n",
    "            img1_path, img2_path, is_draw=False)\n",
    "        res = {\"matches\": matches.tolist(), \"kpts1\": kpts1.tolist(),\n",
    "               \"kpts2\": kpts2.tolist(), \"scores\": scores.tolist()}\n",
    "\n",
    "        # 将monitor向样品对齐\n",
    "        aligned_img, matrix = get_homography_res(\n",
    "            img1_path, img2_path, kpts1, kpts2, matches, is_draw=False)\n",
    "        \n",
    "        if aligned_img is None:\n",
    "            print(f\"{demo}下的第{i}张图 : 匹配点数过少\")\n",
    "            continue\n",
    "        \n",
    "        # transform_coord(demo, i, matrix)\n",
    "\n",
    "        # 保存aligned_img\n",
    "        save_path = f\"/home/hechunjiang/gradio/aligned_imgs/{demo}/{i}.jpg\"\n",
    "        if not os.path.exists(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "\n",
    "        cv2.imwrite(save_path, cv2.cvtColor(aligned_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        croped_image_path1 = crop_image(\n",
    "            save_path, res['kpts2'], 300, demo, \"finetune_ref\")\n",
    "        croped_image_path2 = crop_image(\n",
    "            img2_path, res['kpts2'], 300, demo, \"finetune_dst\")\n",
    "        print(f\"{demo} : {i}.png done, num of croped_image: {len(croped_image_path1)}\")\n",
    "        all_croped_image_counts[idx].append(len(croped_image_path1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 20\n",
    "\n",
    "image2_path = img2_path_list[4]\n",
    "\n",
    "img1_path = f\"/home/hechunjiang/gradio/监视器采集图像/{i}.jpg\"\n",
    "img2_path = f\"{image2_path}{i}.jpg\"\n",
    "\n",
    "g = GeoFormer(640, 0.5, no_match_upscale=False,\n",
    "              ckpt='saved_ckpt/geoformer.ckpt', device='cuda')\n",
    "matches, kpts1, kpts2, scores = g.match_pairs(\n",
    "    img1_path, img2_path, is_draw=True)\n",
    "res = {\"matches\": matches.tolist(), \"kpts1\": kpts1.tolist(),\n",
    "        \"kpts2\": kpts2.tolist(), \"scores\": scores.tolist()}\n",
    "\n",
    "# 将monitor向样品对齐\n",
    "aligned_img, matrix = get_homography_res(\n",
    "    img1_path, img2_path, kpts1, kpts2, matches, is_draw=True)\n",
    "\n",
    "plt.figure(dpi=500)\n",
    "plt.imshow(aligned_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoFormer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
